{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectorisation Notebook\n",
    "Below is the code used for generating various embeddings used in the neural models.\n",
    "\n",
    "### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/nlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "import h5py as h5py\n",
    "\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Embedding,LSTM,Dense,Lambda,merge,Input\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint,Callback\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions defined\n",
    "```python\n",
    "maybe_download(filename)\n",
    "#Downloads a file if not present.\n",
    "\n",
    "read_data(filename)\n",
    "#Extract the first file enclosed in a zip file as a list of words.\n",
    "\n",
    "build_dataset(words, n_words)\n",
    "#Process raw inputs into a dataset.\n",
    "\n",
    "generate_batches(data, size, contextWidth, negativeSize)\n",
    "#Returns batches of input words with their contexts and a set of negative samples.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, filename):\n",
    "  \"\"\"Download a file if not present.\"\"\"\n",
    "  if not os.path.exists(\"./downloads/\"+filename):\n",
    "    filename, _ = urllib.request.urlretrieve(url + filename, \"./downloads/\"+filename)\n",
    "  return filename\n",
    "\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "  with zipfile.ZipFile(\"./downloads/\"+filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "  \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def generate_batches(data, size, contextWidth, negativeSize):\n",
    "  cHalfWidth = int(contextWidth/2)\n",
    "  words = []\n",
    "  contexts = []\n",
    "  negatives = []\n",
    "  index = random.sample(range(cHalfWidth,len(data)-cHalfWidth),size)\n",
    "  for z in index:\n",
    "      context = []\n",
    "      for m in range(-cHalfWidth,cHalfWidth+1):\n",
    "        if m == 0:\n",
    "          words.append([data[z]])\n",
    "        else: \n",
    "          context.append(data[z+m])\n",
    "      contexts.append(context)\n",
    "      negatives.append(random.sample(data,negativeSize))\n",
    "  return([np.array(words),np.array(contexts),np.array(negatives)],[np.array([1]*size),np.array([[0]*negativeSize]*size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Wikipedia text database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = maybe_download('http://mattmahoney.net/dc/', 'text8.zip')\n",
    "vocabulary = read_data(filename)\n",
    "print('Number of words: ', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "data_index = 0\n",
    "batch_size = 128\n",
    "wordvec_dim = 32\n",
    "\n",
    "skip_window = 3       # How many words to consider left and right.\n",
    "num_skips = 4         # How many times to reuse an input to generate a label.\n",
    "context_half = 3\n",
    "context_size = context_half*2\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "neg_size = 5    # Number of negative examples to sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary and reverse dictionary for word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a .tsv label file for viewing in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./logs/word2vec_label.tsv', 'w') as fr:\n",
    "    for i in range(vocabulary_size):\n",
    "        fr.write(reverse_dictionary[i]+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training/validation batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = generate_batches(data, 500000, context_size, neg_size)\n",
    "vX, vY = generate_batches(data, 5000, context_size, neg_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining neural model with Keras\n",
    "Graph of cbow neural model shown below.\n",
    "\n",
    "\n",
    "![Graph of cbow model](./images/cbowGraph.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = Input(shape=(1,), name='inputWord')\n",
    "context = Input(shape=(context_size,), name='inputContext')\n",
    "negSamples = Input(shape=(neg_size,), name='inputNegatives')\n",
    "\n",
    "word2vec = Embedding(input_dim=vocabulary_size,output_dim=wordvec_dim, embeddings_initializer='glorot_normal', name='word2vec')\n",
    "\n",
    "vec_word = word2vec(word)\n",
    "vec_context = word2vec(context)\n",
    "vec_negSamples = word2vec(negSamples)\n",
    "cbow = Lambda(lambda x: K.mean(x, axis=1), name='cbowAverage')(vec_context)\n",
    "\n",
    "word_context = merge([vec_word, cbow], mode='dot')\n",
    "negative_context = merge([vec_negSamples, cbow], mode='dot', concat_axis=-1)\n",
    "\n",
    "model = Model(input=[word,context,negSamples], output=[word_context,negative_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create logs for saving parameters and run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs/wordvec', \n",
    "  batch_size=500, histogram_freq=1, write_images=True, write_grads=False, write_graph=True, embeddings_freq=1)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('./logs/model.hdf5')\n",
    "\n",
    "model.fit(X,Y,epochs=50,batch_size=500,callbacks=[model_checkpoint,tensorboard], validation_data=(vX,vY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
