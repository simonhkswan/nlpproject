{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Modality Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint,Callback\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_PATH = './downloads/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for downloading and unzipping modality data\n",
    "All data is saved to the downloads file so as to not be uploaded to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, name):\n",
    "    \n",
    "    global dl_PATH\n",
    "    \n",
    "    if not os.path.exists(dl_PATH):\n",
    "        os.makedirs(dl_PATH)\n",
    "        \n",
    "    if os.path.isfile(dl_PATH+name):\n",
    "        print(name+' already downloaded.')\n",
    "    else:\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, dl_PATH+name)\n",
    "            print(name+' successfully downloaded.')\n",
    "        except:\n",
    "            print('Error downloading '+name+'.')\n",
    "        \n",
    "def maybe_unzip(zname):\n",
    "    global dl_PATH\n",
    "    \n",
    "    if not os.path.isfile(dl_PATH+'task1_train_bio_abstracts_rev2.xml'):\n",
    "        with zipfile.ZipFile(dl_PATH+zname, 'r') as zipref:\n",
    "            zipref.extractall(dl_PATH)\n",
    "    else:\n",
    "        print(zname+' already unzipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_train_bio.zip already downloaded.\n",
      "task1_train_bio.zip already unzipped.\n"
     ]
    }
   ],
   "source": [
    "maybe_download('http://rgai.inf.u-szeged.hu/~vinczev/conll2010st/task1_train_bio_rev2.zip', 'task1_train_bio.zip')\n",
    "maybe_unzip('task1_train_bio.zip') #'task1_train_bio_abstracts_rev2.xml' and 'task1_train_bio_fullarticles_rev2.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class/functions definitions for handling .xml data files\n",
    "The data is presented in an xml file format and is processed here into an ElementTree. The element tree behaves similarly to a nested list structure with a few extra methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(object):\n",
    "    def __init__(self, xml):\n",
    "        with open(dl_PATH+xml) as fd:\n",
    "            self.ETree = ET.parse(fd)\n",
    "            \n",
    "    def totaldocNo(self):\n",
    "        return(len(self.get_documents()))\n",
    "        \n",
    "    def totsentNo(self):\n",
    "        N = 0\n",
    "        for doc in self.getdocuments():\n",
    "            N += len(doc[2][:])\n",
    "        return(N)\n",
    "    \n",
    "    def get_docs(self, start=None, stop=None):\n",
    "        return(self.ETree.getroot()[0][start:stop])\n",
    "    \n",
    "    def tosent(doc):\n",
    "        return(doc[2][:])\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        sentences = []\n",
    "        for doc in self.get_docs():\n",
    "            for part in doc[1:]:\n",
    "                for sent in part[:]:\n",
    "                    sentences.append(sent)\n",
    "        return(sentences)\n",
    "\n",
    "def toString(sentElement):\n",
    "    sent = sentElement.text\n",
    "    if sent == None:\n",
    "        sent = ''\n",
    "    ccuelen = len(sentElement.getchildren())\n",
    "    if ccuelen > 0:\n",
    "        for i in range(ccuelen):\n",
    "            sent += sentElement[i].text\n",
    "            sent += sentElement[i].tail\n",
    "    return(sent)\n",
    "\n",
    "def toStrings(sentElements):\n",
    "    strings = []\n",
    "    for element in sentElements:\n",
    "        strings.append(toString(element))\n",
    "    return(strings)\n",
    "    \n",
    "def isCertain(sentElement):\n",
    "    if sentElement.attrib['certainty'] == 'certain':\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "    \n",
    "def get_cues(sentElement):\n",
    "    return sentElement.getchildren()\n",
    "\n",
    "def num_words(string):\n",
    "    return len(text_to_word_sequence(string,\n",
    "                                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                     lower=True,\n",
    "                                     split=\" \"))\n",
    "\n",
    "def cue_positions(sentElement):\n",
    "    pos = 0\n",
    "    positions = []\n",
    "    pos += num_words(toString(sentElement))\n",
    "    for cue in get_cues(sentElement)[::-1]:\n",
    "        pos -= num_words(cue.tail)\n",
    "        pos -= num_words(cue.text)\n",
    "        positions.append(pos)\n",
    "    return(positions[::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextData('task1_train_bio_abstracts_rev2.xml')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.get_docs(0,1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.get_sentences()\n",
    "sentence_lengths = []\n",
    "for sentence in toStrings(sentences):\n",
    "    sentence_lengths.append(num_words(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(7,3))\n",
    "ax1 = fig1.gca()\n",
    "ax1.set_title('Sentence Length Distribution')\n",
    "ax1.set_ylabel('Number of sentences')\n",
    "ax1.set_xlabel('Number of words')\n",
    "plt.hist(sentence_lengths,bins=81,range=(0,80),color=[0.7,0.1,0.2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(sentences, maxlen, batchsize):\n",
    "    # Creates a list of input data for training the RNN. \n",
    "    # Each batch contains sentences with lengths binned into mulitples of 10.\n",
    "\n",
    "    max_size = int((maxlen+9)/10)\n",
    "    size_grouped = []\n",
    "    for i in range(max_size):\n",
    "        size_grouped.append([])\n",
    "    for sentence in sentences:\n",
    "        string = toString(sentence)\n",
    "        words = text_to_word_sequence(string,\n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                          lower=True,\n",
    "                          split=\" \")\n",
    "        size = int((len(words)+9)/10)\n",
    "        if size <= max_size:\n",
    "            certainty = [isCertain(sentence)]\n",
    "            size_grouped[size-1].append([words,certainty])\n",
    "\n",
    "    batches = []\n",
    "    \n",
    "    sizehistx = []\n",
    "    sizehisty = []\n",
    "    l = 10\n",
    "    for group in size_grouped:\n",
    "        sizehistx.append(str(l))\n",
    "        l+=10\n",
    "        shuffle(group)\n",
    "        numbatch = int(len(group)/batchsize)+1\n",
    "        sizehisty.append(numbatch)\n",
    "        group.append(group[:len(group)%batchsize+1])\n",
    "        for i in range(numbatch):\n",
    "            batches.append(group[(i)*batchsize:(i+1)*batchsize])\n",
    "    fig2 = plt.figure(figsize=(7,3))\n",
    "    ax2 = fig2.gca()\n",
    "    ax2.set_title('Number of Batches per Sentence Length')\n",
    "    ax2.set_ylabel('Number of batches')\n",
    "    ax2.set_xlabel('Number of words')\n",
    "    plt.bar(sizehistx,sizehisty,color=[0.7,0.1,0.2])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return(shuffle(batches))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batches(sentences,80,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dimension = 50\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file read failed: time = Tue Jan 23 17:55:29 2018\n, filename = './logs/', file descriptor = 56, errno = 21, error message = 'Is a directory', buf = 0x7fff5bcb8c10, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-abd7c2fe30ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mweighted_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m               target_tensors=None)\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./images/modalityLSTMmodel.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/nlp/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/nlp/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/envs/nlp/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file read failed: time = Tue Jan 23 17:55:29 2018\n, filename = './logs/', file descriptor = 56, errno = 21, error message = 'Is a directory', buf = 0x7fff5bcb8c10, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_dimension,name='word2vec',trainable=True))\n",
    "model.add(LSTM(30, \n",
    "               activation='tanh', # activation function used\n",
    "               recurrent_activation='hard_sigmoid', # activation function for recurrent step\n",
    "               use_bias=True, # whether the layer uses a bias vector\n",
    "               kernel_initializer='glorot_uniform', # initialiser for the weights matrix\n",
    "               recurrent_initializer='orthogonal', # initialiser for the recurrent kernal's weights\n",
    "               bias_initializer='zeros', # initialiser for the bias vector\n",
    "               unit_forget_bias=True, # add 1 to the bias of the forget gate at initialization\n",
    "               kernel_regularizer=None, # regularizer function applied to kernal\n",
    "               recurrent_regularizer=None, # regularizer function applied to recurrent kernal\n",
    "               bias_regularizer=None, # regularizer function applied to bias vector\n",
    "               activity_regularizer=None, # regularizer function applied to output of the layer\n",
    "               kernel_constraint=None, # constraint function applied to the kernal\n",
    "               recurrent_constraint=None, # constraint function applied to the recurrent kernal\n",
    "               bias_constraint=None, # constraint function applied to the bias vector\n",
    "               dropout=0.0, # fraction of units to drop for the linear transformation of the inputs\n",
    "               recurrent_dropout=0.0, # fraction of units to drop for the linear transformation of the recurrent state\n",
    "               implementation=1, # implementation mode, either 1 or 2.\n",
    "               return_sequences=False, \n",
    "               return_state=False, \n",
    "               go_backwards=False, \n",
    "               stateful=False, # If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n",
    "               unroll=False)) # whether the network will be unrolled, otherwise a symbolic loop will be used.\n",
    "model.add(Dense(2,\n",
    "                activation='softmax', \n",
    "                use_bias=True, \n",
    "                kernel_initializer='glorot_uniform', \n",
    "                bias_initializer='zeros', \n",
    "                kernel_regularizer=None, \n",
    "                bias_regularizer=None, \n",
    "                activity_regularizer=None, \n",
    "                kernel_constraint=None, \n",
    "                bias_constraint=None))\n",
    "model.compile(optimizer='RMSprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['acc'], \n",
    "              sample_weight_mode=None, \n",
    "              weighted_metrics=None, \n",
    "              target_tensors=None)\n",
    "model.load_weights('./logs/',by_name=True)\n",
    "model.summary()\n",
    "plot_model(model, to_file='./images/modalityLSTMmodel.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graph of LSTM Model](./images/modalityLSTMmodel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs/modality', batch_size=batch_size, \n",
    "                          histogram_freq=1, write_images=True, \n",
    "                          write_grads=False, write_graph=True, embeddings_freq=1)\n",
    "model_checkpoint = ModelCheckpoint('./logs/modalityLSTMmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
