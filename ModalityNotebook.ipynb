{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Modality Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/nlp/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Embedding,LSTM,Dense\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint,Callback\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_PATH = './downloads/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for downloading and unzipping modality data\n",
    "All data is saved to the downloads file so as to not be uploaded to github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url, name):\n",
    "    \n",
    "    global dl_PATH\n",
    "    \n",
    "    if not os.path.exists(dl_PATH):\n",
    "        os.makedirs(dl_PATH)\n",
    "        \n",
    "    if os.path.isfile(dl_PATH+name):\n",
    "        print(name+' already downloaded.')\n",
    "    else:\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, dl_PATH+name)\n",
    "            print(name+' successfully downloaded.')\n",
    "        except:\n",
    "            print('Error downloading '+name+'.')\n",
    "        \n",
    "def maybe_unzip(zname):\n",
    "    global dl_PATH\n",
    "    \n",
    "    if not os.path.isfile(dl_PATH+'task1_train_bio_abstracts_rev2.xml'):\n",
    "        with zipfile.ZipFile(dl_PATH+zname, 'r') as zipref:\n",
    "            zipref.extractall(dl_PATH)\n",
    "    else:\n",
    "        print(zname+' already unzipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_train_bio.zip already downloaded.\n",
      "task1_train_bio.zip already unzipped.\n"
     ]
    }
   ],
   "source": [
    "maybe_download('http://rgai.inf.u-szeged.hu/~vinczev/conll2010st/task1_train_bio_rev2.zip', 'task1_train_bio.zip')\n",
    "maybe_unzip('task1_train_bio.zip') #'task1_train_bio_abstracts_rev2.xml' and 'task1_train_bio_fullarticles_rev2.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class/functions definitions for handling .xml data files\n",
    "The data is presented in an xml file format and is processed here into an ElementTree. The element tree behaves similarly to a nested list structure with a few extra methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(object):\n",
    "    def __init__(self, xml):\n",
    "        with open(dl_PATH+xml) as fd:\n",
    "            self.ETree = ET.parse(fd)\n",
    "            \n",
    "    def totaldocNo(self):\n",
    "        return(len(self.get_documents()))\n",
    "        \n",
    "    def totsentNo(self):\n",
    "        N = 0\n",
    "        for doc in self.getdocuments():\n",
    "            N += len(doc[2][:])\n",
    "        return(N)\n",
    "    \n",
    "    def get_docs(self, start=None, stop=None):\n",
    "        return(self.ETree.getroot()[0][start:stop])\n",
    "    \n",
    "    def tosent(doc):\n",
    "        return(doc[2][:])\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        sentences = []\n",
    "        for doc in self.get_docs():\n",
    "            for part in doc[1:]:\n",
    "                for sent in part[:]:\n",
    "                    sentences.append(sent)\n",
    "        return(sentences)\n",
    "\n",
    "def toString(sentElement):\n",
    "    sent = sentElement.text\n",
    "    ccuelen = len(sentElement.getchildren())\n",
    "    if ccuelen > 0:\n",
    "        for i in range(ccuelen):\n",
    "            sent += sentElement[i].text\n",
    "            sent += sentElement[i].tail\n",
    "    return(sent)\n",
    "\n",
    "def toStrings(sentElements):\n",
    "    strings = []\n",
    "    for element in sentElements:\n",
    "        strings.append(toString(element))\n",
    "    \n",
    "def isCertain(sentElement):\n",
    "    if sentElement.attrib['certainty'] == 'certain':\n",
    "        return(True)\n",
    "    else:\n",
    "        return(False)\n",
    "    \n",
    "def get_cues(sentElement):\n",
    "    return sentElement.getchildren()\n",
    "\n",
    "def num_words(string):\n",
    "    return len(string.replace('/',' ').split())\n",
    "\n",
    "def cue_positions(sentElement):\n",
    "    pos = 0\n",
    "    positions = []\n",
    "    pos += num_words(toString(sentElement))\n",
    "    for cue in get_cues(sentElement)[::-1]:\n",
    "        pos -= num_words(cue.tail)\n",
    "        pos -= num_words(cue.text)\n",
    "        positions.append(pos)\n",
    "    return(positions[::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextData('task1_train_bio_abstracts_rev2.xml')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'Document' at 0x10c424d68>\n"
     ]
    }
   ],
   "source": [
    "print(data.get_docs(0,1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.get_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "Electrophoretic mobility shift assays and Southwestern blotting experiments were used to detect the binding of cellular transactivation factor NF-KB to the double repeat-KB enhancer sequence located in the long terminal repeat.\n",
      "[]\n",
      "[]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(num_words(toString(sentences[2])))\n",
    "print(toString(sentences[2]))\n",
    "print(get_cues(sentences[2]))\n",
    "print(cue_positions(sentences[2]))\n",
    "print(isCertain(sentences[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
